{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Sep 17 14:49:49 2019\n\n@author: Samira\n\"\"\"\n#Leaderboard score: 0.92296\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom keras.layers import Dense, Input\nfrom keras.layers import Conv2D, Flatten, Lambda\nfrom keras.layers import Reshape, Conv2DTranspose\nfrom keras.models import Model\nfrom keras.datasets import mnist\nfrom keras.losses import mse, binary_crossentropy\nfrom keras.utils import plot_model\nfrom keras import backend as K\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport argparse\nimport os \nfrom sklearn.preprocessing import minmax_scale\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# reparameterization trick\n# instead of sampling from Q(z|X), sample eps = N(0,I)\n# then z = z_mean + sqrt(var)*eps\ndef sampling(args):\n    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n    # Arguments\n        args (tensor): mean and log of variance of Q(z|X)\n\n    # Returns\n        z (tensor): sampled latent vector\n    \"\"\"\n\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    # by default, random_normal has mean=0 and std=1.0\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_results(models,\n                 data,\n                 batch_size=128,\n                 model_name=\"vae_mnist\"):\n    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n\n    # Arguments\n        models (tuple): encoder and decoder models\n        data (tuple): test data and label\n        batch_size (int): prediction batch size\n        model_name (string): which model is using this function\n    \"\"\"\n\n    encoder, decoder = models\n    x_test, y_test = data\n    os.makedirs(model_name, exist_ok=True)\n\n    filename = os.path.join(model_name, \"vae_mean.png\")\n    # display a 2D plot of the digit classes in the latent space\n    z_mean, _, _ = encoder.predict(x_test,\n                                   batch_size=batch_size)\n    \n    plt.figure(figsize=(12, 10))\n    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n    plt.colorbar()\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.savefig(filename)\n    plt.show()\n    \n    filename = os.path.join(model_name, \"digits_over_latent.png\")\n    \n    # display a 10x102D manifold of waves\n    \n    n = 10\n    grid_x = np.linspace(-4, 4, n)\n    grid_y = np.linspace(-4, 4, n)[::-1]\n    fig = plt.figure(figsize=(100, 100))\n\n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z_sample = np.array([[xi, yi]])\n            x_decoded = decoder.predict(z_sample)\n            #print(x_decoded.shape)\n            digit = x_decoded[0]\n            #temp = np.reshape(x_decoded, (44944, 1))\n            #plt.plot(temp)\n            plt.subplot(n,n, i*n + j+1)\n            plt.plot(np.reshape(digit/digit.mean(), (44944,1)))\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train = np.load(\"/kaggle/input/audio-binary-classification/train_data.npy\")\nX_test = np.load(\"/kaggle/input/audio-binary-classification/test_data.npy\")\ny_train = pd.read_csv('/kaggle/input/audio-binary-classification/train_labels.csv').loc[:,'Label'].values\n\n\"\"\"\ntrain_scaled = minmax_scale(x_train, axis = 0)\ntest_scaled = minmax_scale(x_test, axis = 0)\n\"\"\"\nimage_size = int(math.sqrt(x_train.shape[1]))+2\ndesire_size = int(math.pow(212, 2))\noriginal_size = x_train.shape[1]\npad_size = desire_size - x_train.shape[1]\nx_train = np.c_[ x_train,   np.zeros((x_train.shape[0], pad_size))] \nx_test = np.c_[x_test, np.zeros((x_test.shape[0], pad_size))] \n#x_train = sc.fit_transform(x_train)\n#x_test = sc.transform(x_test)\nx_train = np.reshape(x_train, (-1, image_size, image_size, 1))\nx_test = np.reshape(x_test, (-1, image_size, image_size, 1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# network parameters\ninput_shape = (image_size, image_size, 1)\nbatch_size = 128\nkernel_size = 3\nfilters = 16\nlatent_dim = 30\nepochs = 50\n# VAE model = encoder + decoder\n# build encoder model\ninputs = Input(shape=input_shape, name='encoder_input')\nx = inputs\nfor i in range(2):\n    filters *= 2\n    x = Conv2D(filters=filters,\n               kernel_size=kernel_size,\n               activation='relu',\n               strides=2,\n               padding='same')(x)\n\n# shape info needed to build decoder model\nshape = K.int_shape(x)\n\n# generate latent vector Q(z|X)\nx = Flatten()(x)\nx = Dense(16, activation='relu')(x)\nz_mean = Dense(latent_dim, name='z_mean')(x)\nz_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n# use reparameterization trick to push the sampling out as input\n# note that \"output_shape\" isn't necessary with the TensorFlow backend\nz = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n# instantiate encoder model\nencoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\nencoder.summary()\nplot_model(encoder, to_file='vae_cnn_encoder.png', show_shapes=True)\n# build decoder model\nlatent_inputs = Input(shape=(latent_dim,), name='z_sampling')\nx = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs)\nx = Reshape((shape[1], shape[2], shape[3]))(x)\n\nfor i in range(2):\n    x = Conv2DTranspose(filters=filters,\n                        kernel_size=kernel_size,\n                        activation='relu',\n                        strides=2,\n                        padding='same')(x)\n    filters //= 2\n\noutputs = Conv2DTranspose(filters=1,\n                          kernel_size=kernel_size,\n                          activation='sigmoid',\n                          padding='same',\n                          name='decoder_output')(x)\n\n# instantiate decoder model\ndecoder = Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()\nplot_model(decoder, to_file='vae_cnn_decoder.png', show_shapes=True)\n# instantiate VAE model\noutputs = decoder(encoder(inputs)[2])\nvae = Model(inputs, outputs, name='vae')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nmodels = (encoder, decoder)\n#data = (x_test, y_test)\n\n# VAE loss = mse_loss or xent_loss + kl_loss\nreconstruction_loss = binary_crossentropy(K.flatten(inputs),\n                                              K.flatten(outputs))\n\nreconstruction_loss *= image_size * image_size\nkl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n\nkl_loss = K.sum(kl_loss, axis=-1)\nkl_loss *= -0.5\nvae_loss = K.mean(reconstruction_loss + kl_loss)\n\nvae.add_loss(vae_loss)\nvae.compile(optimizer='rmsprop')\nvae.summary()\n\nplot_model(vae, to_file='vae_cnn_latent2.png', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train the autoencoder\nvae.fit(x_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_data=(x_test, None))\nvae.save_weights('vae_cnn_mnist.h5')\n#plot_results(models, data, batch_size=batch_size, model_name=\"vae_cnn\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"encoder, decoder = models\n\n_, _, encoded_train = encoder.predict(x_train)\n#pd.DataFrame() \n_, _, encoded_test = encoder.predict(x_test) \n# display a 2D plot of the digit classes in the latent space\n#z_mean, z_log_var, z_test = encoder.predict(x_test,batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(encoded_train.shape)\nprint(encoded_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(encoded_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict_proba(encoded_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.DataFrame(y_pred[:,1], index = range(y_pred.shape[0]))\ndf.reset_index(level=0, inplace=True)\ndf.columns=['Id', 'Label']\ndf.to_csv(\"y_pred_VAE_30_RF.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}